{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SccdjGX_5nXX",
        "cNIbsI7tG3Kp",
        "62lBwNykIUtn",
        "tl3IXvuY6osX",
        "EF-yZh2JKJmH",
        "MUi_bLvXKZk_",
        "atwKgTuCKnqE",
        "ScMEdKavKt2D",
        "9JTsBHtrK1In",
        "4X4lAeLF9Z9v"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Top Quark Training Tutorial(Hybrid_Effswin-Architecture)\n",
        "This notebook will give the reader a basic idea on how train the Hybrid_EffSwin(TPU) architecture to get somewhat similar results as described in the article.\n",
        "\n",
        "Framework: Tensorflow</br>\n",
        "Accelerator: TPU</br>\n",
        "The following notebook demonstrates the training on a TPU(V3-8), it is assumed that the reader has sufficient knowledge of TFRecords and Tensorflow to follow this tutorial.</br></br>\n",
        "Note: This tutorial will only show how the first stage of the model is trained since training the second stage is complicated on Collab due to Memory Limitations. For the full training script please refer to the training repository\n"
      ],
      "metadata": {
        "id": "FaF9V1PG2YpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "SccdjGX_5nXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dc250601/GSOC.git\n",
        "%mv GSOC Transformers\n",
        "%mkdir /content/Checkpoints"
      ],
      "metadata": {
        "id": "EpgdBw-r2yiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a9cbdc-3904-40ef-c319-a980686c77be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GSOC'...\n",
            "remote: Enumerating objects: 246, done.\u001b[K\n",
            "remote: Counting objects: 100% (246/246), done.\u001b[K\n",
            "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
            "remote: Total 246 (delta 88), reused 185 (delta 47), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (246/246), 519.50 KiB | 5.36 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!pip install sklearn\n",
        "!pip install tqdm\n",
        "!pip install timm\n",
        "!pip install tensorflow_addons\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGQnksrQ48mB",
        "outputId": "9c883c90-907a-4cf1-b5cb-b3ad8d5cee6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.4-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.28-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 21.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 27.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 40.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 9.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 30.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 27.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 28.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 20.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 11.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 26.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 32.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 13.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=0474749540d5e5fbc9365c3eea4307f7440c8bc69b98412dc5c47e64e3377850\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.28 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=d883fd20b830240e83bd2923be2eb5ce39148a9aadd12bd582aa52789439cfb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from timm) (6.0)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->timm) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (5.0.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub->timm) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.10.0 timm-0.6.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.18.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.5.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to the TPU"
      ],
      "metadata": {
        "id": "cNIbsI7tG3Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "#Get a handle to the attached TPU. On GCP it will be the CloudTPU itself\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"])\n",
        "#Connect to the TPU handle and initialise it\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvkFilj9Gp45",
        "outputId": "3b530d5a-6b5c-4a0e-f654-f623d19ff041"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "Ij9L38OZ5a_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Transformers\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "import tensorflow_addons as tfa\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "import wandb\n",
        "import gc\n",
        "#------------------------------\n",
        "from keras import backend\n",
        "from keras.distribute import distributed_file_utils\n",
        "from keras.distribute import worker_training_state\n",
        "# from keras.optimizers.schedules import learning_rate_schedule\n",
        "from keras.utils import generic_utils\n",
        "from keras.utils import io_utils\n",
        "from keras.utils import tf_utils\n",
        "from keras.utils import version_utils\n",
        "from keras.utils.data_utils import Sequence\n",
        "from keras.utils.generic_utils import Progbar\n",
        "from keras.utils.mode_keys import ModeKeys\n",
        "import numpy as np\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "from tensorflow.python.util import deprecation  # pylint: disable=g-direct-tensorflow-import\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "from tensorflow.tools.docs import doc_controls\n",
        "try:\n",
        "  import requests\n",
        "except ImportError:\n",
        "  requests = None\n",
        "\n",
        "import Model.Top_Quark.effswin as effswin"
      ],
      "metadata": {
        "id": "x2PmQNjN5jZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406bd518-6af5-4b33-c06c-96959d327007"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up Weights And Biases for easier visualization"
      ],
      "metadata": {
        "id": "1SYYsP3lIqcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "wandb.init(\n",
        "      project = \"Tutorial_Transformers\",\n",
        "      name = \"HybridEffSwin_TPU\"\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "i36riZxpIfSN",
        "outputId": "d97af361-0578-4a30-e785-dcf1400fcb3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdc250601\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Transformers/wandb/run-20221010_001838-2msm9mue</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/dc250601/Tutorial_Transformers/runs/2msm9mue\" target=\"_blank\">HybridEffSwin_TPU</a></strong> to <a href=\"https://wandb.ai/dc250601/Tutorial_Transformers\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dc250601/Tutorial_Transformers/runs/2msm9mue?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fc8cbe3ed50>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configs"
      ],
      "metadata": {
        "id": "62lBwNykIUtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128 * tpu_strategy.num_replicas_in_sync\n",
        "SHUFFLE_BUFFER = 2048*6\n",
        "LR = 5e-4\n",
        "WD = 1e-5\n",
        "LR_2 = 1e-3\n",
        "WD_2 = 5e-5\n",
        "ROT_ANGLE = 20\n",
        "GCS_DS_PATH = \"gs://top_dataset_us/TFR\"\n",
        "VAL_STEPS = 32*4\n",
        "TRAIN_STEPS = 194*4*5\n",
        "EPOCHS = 100\n",
        "WARMUP_EPOCHS = 1\n",
        "BATCH_SHUFFLE_BUFFER = 10\n",
        "wandb.config.update({\"learning_rate_stage1\":LR,\n",
        "                    \"weight_decay_stage1\":WD,\n",
        "                    \"learning_rate_stage2\":LR_2,\n",
        "                    \"weight_decay_stage2\":WD_2,\n",
        "                    \"Rotation Angle\":ROT_ANGLE,\n",
        "                    \"Shuffle_buffer size\":SHUFFLE_BUFFER,\n",
        "                    \"BATCH_SIZE\":BATCH_SIZE,\n",
        "                    \"Validation steps\":VAL_STEPS,\n",
        "                    \"Training Steps\":TRAIN_STEPS,\n",
        "                    \"Training Epochs\":EPOCHS,\n",
        "                    \"WarmUp_Epochs\":WARMUP_EPOCHS,\n",
        "                    \"BATCH_SHUFFLE_BUFFER\":BATCH_SHUFFLE_BUFFER\n",
        "                    })"
      ],
      "metadata": {
        "id": "guiVBBw_ITZd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up TFRecord reader"
      ],
      "metadata": {
        "id": "S8GDgYjsH8nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILES = tf.io.gfile.glob(GCS_DS_PATH + '/*.tfrecords')\n",
        "TRAIN = FILES[:250]\n",
        "TEST = FILES[250:300]\n",
        "\n",
        "\n",
        "def _parse_image_label_function(example_proto):\n",
        "    # Create a dictionary describing the features.\n",
        "    image_feature_description = {\n",
        "        'channel_1': tf.io.FixedLenFeature([], tf.string),\n",
        "        'channel_2': tf.io.FixedLenFeature([], tf.string),\n",
        "        'channel_3': tf.io.FixedLenFeature([], tf.string),\n",
        "        'channel_4': tf.io.FixedLenFeature([], tf.string),\n",
        "        'channel_5': tf.io.FixedLenFeature([], tf.string),\n",
        "        'channel_6': tf.io.FixedLenFeature([], tf.string),\n",
        "        'channel_7': tf.io.FixedLenFeature([], tf.string),\n",
        "        'channel_8': tf.io.FixedLenFeature([], tf.string),\n",
        "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'name': tf.io.FixedLenFeature([], tf.string)}\n",
        "\n",
        "    # Parse the input tf.train.Example proto using the dictionary above.\n",
        "    content = tf.io.parse_single_example(example_proto, image_feature_description)\n",
        "    content[\"channel_1\"] = tf.io.decode_png(content[\"channel_1\"], channels=1)\n",
        "    content[\"channel_2\"] = tf.io.decode_png(content[\"channel_2\"], channels=1)\n",
        "    content[\"channel_3\"] = tf.io.decode_png(content[\"channel_3\"], channels=1)\n",
        "    content[\"channel_4\"] = tf.io.decode_png(content[\"channel_4\"], channels=1)\n",
        "    content[\"channel_5\"] = tf.io.decode_png(content[\"channel_5\"], channels=1)\n",
        "    content[\"channel_6\"] = tf.io.decode_png(content[\"channel_6\"], channels=1)\n",
        "    content[\"channel_7\"] = tf.io.decode_png(content[\"channel_7\"], channels=1)\n",
        "    content[\"channel_8\"] = tf.io.decode_png(content[\"channel_8\"], channels=1)\n",
        "    content[\"channel_1\"] = tf.cast(content[\"channel_1\"], tf.float32) / 255.0\n",
        "    content[\"channel_2\"] = tf.cast(content[\"channel_2\"], tf.float32) / 255.0\n",
        "    content[\"channel_3\"] = tf.cast(content[\"channel_3\"], tf.float32) / 255.0\n",
        "    content[\"channel_4\"] = tf.cast(content[\"channel_4\"], tf.float32) / 255.0\n",
        "    content[\"channel_5\"] = tf.cast(content[\"channel_5\"], tf.float32) / 255.0\n",
        "    content[\"channel_6\"] = tf.cast(content[\"channel_6\"], tf.float32) / 255.0\n",
        "    content[\"channel_7\"] = tf.cast(content[\"channel_7\"], tf.float32) / 255.0\n",
        "    content[\"channel_8\"] = tf.cast(content[\"channel_8\"], tf.float32) / 255.0\n",
        "    label = content[\"label\"]\n",
        "    return tf.concat([ content[\"channel_1\"],\n",
        "                       content[\"channel_2\"],\n",
        "                       content[\"channel_3\"],\n",
        "                       content[\"channel_4\"],\n",
        "                       content[\"channel_5\"],\n",
        "                       content[\"channel_6\"],\n",
        "                       content[\"channel_7\"],\n",
        "                       content[\"channel_8\"]],axis = -1),label\n",
        "\n",
        "def data_augment_train(image, label):\n",
        "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
        "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
        "    # of the TPU while the TPU itself is computing gradients.\n",
        "    image = tf.image.resize(image, (224,224), method = \"bicubic\")\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    rot_angle = ROT_ANGLE\n",
        "    image = tfa.image.rotate(image,rot_angle*(np.random.rand()-0.5)*2)\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def data_augment_test(image, label):\n",
        "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
        "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
        "    # of the TPU while the TPU itself is computing gradients.\n",
        "    image = tf.image.resize(image, (224,224), method = \"bicubic\")\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def load_dataset(filenames,train = False):\n",
        "#     ignore_order = tf.data.Options()\n",
        "#     ignore_order.experimental_deterministic = False\n",
        "#     dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
        "#     dataset = dataset.with_options(ignore_order)\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset.list_files(filenames)\n",
        "    dataset = dataset.shuffle(buffer_size=1000)\n",
        "    dataset = dataset.interleave(\n",
        "        tf.data.TFRecordDataset,\n",
        "        cycle_length=AUTO,\n",
        "        num_parallel_calls=AUTO,\n",
        "        deterministic=False,\n",
        "        block_length=1)\n",
        "    dataset = dataset.map(_parse_image_label_function, num_parallel_calls=AUTO)\n",
        "    if train:\n",
        "        dataset = dataset.map(data_augment_train, num_parallel_calls = AUTO)\n",
        "        dataset = dataset.repeat()\n",
        "        dataset = dataset.shuffle(SHUFFLE_BUFFER)\n",
        "        dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "        dataset = dataset.shuffle(BATCH_SHUFFLE_BUFFER)\n",
        "    else:\n",
        "        dataset = dataset.map(data_augment_test, num_parallel_calls=AUTO)\n",
        "        dataset = dataset.repeat()\n",
        "        dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "train_ds = load_dataset(TRAIN, train = True)\n",
        "val_ds = load_dataset(TEST, train = False)\n"
      ],
      "metadata": {
        "id": "tmZmsBpeIE2R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the helper functions and classes"
      ],
      "metadata": {
        "id": "tl3IXvuY6osX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AUC Calculator Callback\n"
      ],
      "metadata": {
        "id": "EF-yZh2JKJmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class auc_sklearn(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, val_ds, val_steps):\n",
        "        super(auc_sklearn, self).__init__()\n",
        "        self.val_ds = val_ds\n",
        "        self.val_steps = val_steps\n",
        "        self.auc_hist =[]\n",
        "        self.auc_last = 0\n",
        "        self.loss_last = 0\n",
        "        self.loss_hist = []\n",
        "    def on_train_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Starting training; got log keys: {}\".format(keys))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        @tf.function\n",
        "        def valid_step(images, labels):\n",
        "            probabilities = model(images, training=False)\n",
        "            loss = loss_fn(labels, probabilities)\n",
        "            valid_loss.update_state(loss)\n",
        "            return labels,probabilities\n",
        "\n",
        "        i = 0\n",
        "        step = self.val_steps\n",
        "        l = np.array([], dtype=np.int64).reshape(0)\n",
        "        p = np.array([], dtype=np.float32).reshape(0)\n",
        "        for image, labels in self.val_ds:\n",
        "            labels,probability = tpu_strategy.run(valid_step, args=(image, labels))\n",
        "            for j in range(tpu_strategy.num_replicas_in_sync):\n",
        "                l = np.concatenate([l,labels.values[j].numpy()])\n",
        "                p = np.concatenate([p,tf.squeeze(probability.values[j]).numpy()])\n",
        "            i = i+1\n",
        "            if i == step:\n",
        "                break\n",
        "        auc = roc_auc_score(l,p)\n",
        "        print(f\"{epoch+1} Epoch Ended, Val_loss:{valid_loss.result().numpy()/self.val_steps}, Val_auc:{auc}\")\n",
        "\n",
        "        self.auc_hist.append(auc)\n",
        "        self.auc_last = auc\n",
        "        self.loss_last = valid_loss.result().numpy()/self.val_steps\n",
        "        self.loss_hist.append(self.loss_last)\n",
        "        valid_loss.reset_states()\n"
      ],
      "metadata": {
        "id": "z24sDaifKOA-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduce Learning rate on Plateau callback"
      ],
      "metadata": {
        "id": "MUi_bLvXKZk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReduceLROnPlateau(tf.keras.callbacks.Callback):\n",
        "  \"\"\"Hardcoded DO NOT CHANGE  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               warmup_epochs,\n",
        "               factor=0.1,\n",
        "               patience=10,\n",
        "               verbose=0,\n",
        "               mode='auto',\n",
        "               min_delta=1e-4,\n",
        "               cooldown=0,\n",
        "               min_lr=0,\n",
        "               **kwargs):\n",
        "    super(ReduceLROnPlateau, self).__init__()\n",
        "\n",
        "    if factor >= 1.0:\n",
        "      raise ValueError(\n",
        "          f'ReduceLROnPlateau does not support a factor >= 1.0. Got {factor}')\n",
        "    if 'epsilon' in kwargs:\n",
        "      min_delta = kwargs.pop('epsilon')\n",
        "      logging.warning('`epsilon` argument is deprecated and '\n",
        "                      'will be removed, use `min_delta` instead.')\n",
        "    self.factor = factor\n",
        "    self.min_lr = min_lr\n",
        "    self.min_delta = min_delta\n",
        "    self.patience = patience\n",
        "    self.verbose = verbose\n",
        "    self.cooldown = cooldown\n",
        "    self.cooldown_counter = 0  # Cooldown counter.\n",
        "    self.wait = 0\n",
        "    self.best = 0\n",
        "    self.mode = mode\n",
        "    self.monitor_op = None\n",
        "    self.warmup_epochs = warmup_epochs\n",
        "    self._reset()\n",
        "\n",
        "  def _reset(self):\n",
        "    \"\"\"Resets wait counter and cooldown counter.\n",
        "    \"\"\"\n",
        "    if self.mode not in ['auto', 'min', 'max']:\n",
        "      logging.warning('Learning rate reduction mode %s is unknown, '\n",
        "                      'fallback to auto mode.', self.mode)\n",
        "      self.mode = 'auto'\n",
        "    if (self.mode == 'min' or\n",
        "        (self.mode == 'auto' and 'acc' not in self.monitor)):\n",
        "      self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n",
        "      self.best = np.Inf\n",
        "    else:\n",
        "      self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n",
        "      self.best = -np.Inf\n",
        "    self.cooldown_counter = 0\n",
        "    self.wait = 0\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    self._reset()\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    logs = logs or {}\n",
        "    logs['lr'] = backend.get_value(self.model.optimizer.lr)\n",
        "    current = auc.auc_last\n",
        "    if epoch == self.warmup_epochs:\n",
        "        print(\"Warmup Completed Decay started\")\n",
        "    if epoch >= self.warmup_epochs:\n",
        "      if self.in_cooldown():\n",
        "        self.cooldown_counter -= 1\n",
        "        self.wait = 0\n",
        "\n",
        "      if self.monitor_op(current, self.best):\n",
        "        self.best = current\n",
        "        self.wait = 0\n",
        "      elif not self.in_cooldown():\n",
        "        self.wait += 1\n",
        "        if self.wait >= self.patience:\n",
        "          old_lr = backend.get_value(self.model.optimizer.lr)\n",
        "          old_wd = backend.get_value(self.model.optimizer.weight_decay)\n",
        "          if old_lr > np.float32(self.min_lr):\n",
        "            new_lr = old_lr * self.factor\n",
        "            new_wd = old_wd * self.factor\n",
        "            new_lr = max(new_lr, self.min_lr)\n",
        "            backend.set_value(self.model.optimizer.lr, new_lr)\n",
        "            backend.set_value(self.model.optimizer.weight_decay, new_wd)\n",
        "            if self.verbose > 0:\n",
        "              print(f'\\nEpoch {epoch +1}:ReduceLROnPlateau reducing learning rate to {new_lr}., weight_decay to {new_wd}')\n",
        "            self.cooldown_counter = self.cooldown\n",
        "            self.wait = 0\n",
        "\n",
        "  def in_cooldown(self):\n",
        "    return self.cooldown_counter > 0\n"
      ],
      "metadata": {
        "id": "NXs82bREKiEf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Warmup Callback"
      ],
      "metadata": {
        "id": "atwKgTuCKnqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearWarmUp(tf.keras.callbacks.Callback):\n",
        "  \"\"\"Hardcoded DO NOT CHANGE  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               steps_per_epoch,\n",
        "               warmup_epochs,\n",
        "               max_lr,\n",
        "               max_wd,\n",
        "               min_lr,\n",
        "               min_wd\n",
        "              ):\n",
        "    super(LinearWarmUp, self).__init__()\n",
        "\n",
        "    self.steps_per_epoch = steps_per_epoch\n",
        "    self.warmup_epochs = warmup_epochs\n",
        "    self.max_lr = max_lr\n",
        "    self.min_lr = min_lr\n",
        "    self.max_wd = max_wd\n",
        "    self.min_wd = min_wd\n",
        "    self.total_steps = self.steps_per_epoch*self.warmup_epochs\n",
        "    self.step = 0\n",
        "    self.curr_lr = self.min_lr\n",
        "    self.curr_wd = self.min_wd\n",
        "    self.delta_lr = 0\n",
        "    self.delta_wd = 0\n",
        "\n",
        "  def _reset(self):\n",
        "    self.step = 0\n",
        "    self.delta_lr = (self.max_lr - self.min_lr)/(self.total_steps)\n",
        "    self.delta_wd = (self.max_wd - self.min_wd)/(self.total_steps)\n",
        "    self.curr_lr = self.min_lr\n",
        "    self.curr_wd = self.min_wd\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    self._reset()\n",
        "    print(\"WarmUp started\")\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "    self.step = self.step + 1\n",
        "    if self.step < self.total_steps:\n",
        "        new_lr = self.min_lr + (self.step)*self.delta_lr\n",
        "        new_wd = self.min_wd + (self.step)*self.delta_wd\n",
        "        self.curr_lr = new_lr\n",
        "        self.curr_wd = new_wd\n",
        "        backend.set_value(self.model.optimizer.lr, new_lr)\n",
        "        backend.set_value(self.model.optimizer.weight_decay, new_wd)\n"
      ],
      "metadata": {
        "id": "fCx-2uc8Kq34"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WandB logger Callback"
      ],
      "metadata": {
        "id": "ScMEdKavKt2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class logger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "      super(logger, self).__init__()\n",
        "\n",
        "    def on_epoch_end(self,epoch, logs=None):\n",
        "        wandb.log({\"AUC\": logs[\"auc\"],\n",
        "                   \"Epoch\": epoch,\n",
        "                   \"Val_auc\": auc.auc_last,\n",
        "                    \"Val_loss\":auc.loss_last,\n",
        "                   \"loss\": logs[\"loss\"],\n",
        "                   \"learning_rate\": backend.get_value(self.model.optimizer.lr),\n",
        "                   \"weight_decay\": backend.get_value(self.model.optimizer.weight_decay)})"
      ],
      "metadata": {
        "id": "u5r1fuhzKxli"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checkpoint saving callback"
      ],
      "metadata": {
        "id": "9JTsBHtrK1In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Checkpoint(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, target):\n",
        "        '''\n",
        "        Tensorflow Callback to only save the best weights which are above\n",
        "        certain threshold.\n",
        "        args:\n",
        "        target:The minimum value above which the checkpoints will be saved.\n",
        "        '''\n",
        "        super(Checkpoint, self).__init__()\n",
        "        self.best = target\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if auc.auc_last >= self.best:\n",
        "            self.best = auc.auc_last\n",
        "            wandb.alert(title=\"Crossed the target\",\n",
        "                        text=f\"The current validation auc is {auc.auc_last}\")\n",
        "            with tpu_strategy.scope():\n",
        "                checkpoint = tf.train.Checkpoint(model=model)\n",
        "                checkpoint.save(\"/contents/Checkpoints/Weight\")\n"
      ],
      "metadata": {
        "id": "HseRZB-86nlr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xE7vIpjMKUXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model\n",
        "We wil be building the model and also initialising important elements which are required for training."
      ],
      "metadata": {
        "id": "ReA3S6Nh6zOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tpu_strategy.scope():\n",
        "\n",
        "    model = effswin.Stage1()\n",
        "\n",
        "    model.compile(\n",
        "    optimizer=tfa.optimizers.AdamW(learning_rate=LR, weight_decay=WD),\n",
        "        # steps_per_execution = 32,\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\", \"AUC\"])\n",
        "\n",
        "    valid_loss = tf.keras.metrics.Sum(name=\"val_accuracy\")\n",
        "    loss_fn = lambda a,b: tf.nn.compute_average_loss(\n",
        "        tf.keras.losses.binary_crossentropy(tf.reshape(a, (-1,1)),b, from_logits=True), global_batch_size=BATCH_SIZE)\n",
        "    model.build((1,224,224,8))\n"
      ],
      "metadata": {
        "id": "d4womc9zLG_d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WujSGYIRrk4",
        "outputId": "25d42d50-247c-4164-a2af-b515fcc39657"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"stage1_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " swin_transformer_2 (SwinTra  multiple                 1499970   \n",
            " nsformer)                                                       \n",
            "                                                                 \n",
            " swin_transformer_3 (SwinTra  multiple                 26027064  \n",
            " nsformer)                                                       \n",
            "                                                                 \n",
            " dense_55 (Dense)            multiple                  769       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27,527,803\n",
            "Trainable params: 27,527,803\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the callbacks"
      ],
      "metadata": {
        "id": "CA7uSqEsLnJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auc = auc_sklearn(val_ds=tpu_strategy.experimental_distribute_dataset(val_ds), val_steps=VAL_STEPS)\n",
        "lr_sc = ReduceLROnPlateau(\n",
        "    monitor=auc.auc_last,\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    mode='max',\n",
        "    min_delta=0.0001,\n",
        "    cooldown=0,\n",
        "    min_lr=1e-7,\n",
        "    warmup_epochs=WARMUP_EPOCHS)\n",
        "\n",
        "lr_warm = LinearWarmUp(steps_per_epoch=TRAIN_STEPS,\n",
        "                       warmup_epochs=WARMUP_EPOCHS,\n",
        "                       max_lr=LR,\n",
        "                       max_wd=WD,\n",
        "                       min_lr=0,\n",
        "                       min_wd=0)\n",
        "log = logger()\n"
      ],
      "metadata": {
        "id": "Kdaa20KpLmZf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally, Model.fit !!"
      ],
      "metadata": {
        "id": "ATjTHOV-L0Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_ds,\n",
        "                    steps_per_epoch=TRAIN_STEPS,\n",
        "                    callbacks=[auc, lr_warm, lr_sc, log],\n",
        "                    epochs=EPOCHS\n",
        "                    )\n"
      ],
      "metadata": {
        "id": "HnCk0zprLzXX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wow, you have just trained a transformer model on a TPU. \n",
        "Congrats if you reached this far..."
      ],
      "metadata": {
        "id": "4X4lAeLF9Z9v"
      }
    }
  ]
}