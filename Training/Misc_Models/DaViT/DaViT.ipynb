{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4edf0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "\n",
    "class MySequential(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        for module in self._modules.values():\n",
    "            if type(inputs) == tuple:\n",
    "                inputs = module(*inputs)\n",
    "            else:\n",
    "                inputs = module(inputs)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features,\n",
    "            hidden_features=None,\n",
    "            out_features=None,\n",
    "            act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvPosEnc(nn.Module):\n",
    "    def __init__(self, dim, k=3, act=False, normtype=False):\n",
    "        super(ConvPosEnc, self).__init__()\n",
    "        self.proj = nn.Conv2d(dim,\n",
    "                              dim,\n",
    "                              to_2tuple(k),\n",
    "                              to_2tuple(1),\n",
    "                              to_2tuple(k // 2),\n",
    "                              groups=dim)\n",
    "        self.normtype = normtype\n",
    "        if self.normtype == 'batch':\n",
    "            self.norm = nn.BatchNorm2d(dim)\n",
    "        elif self.normtype == 'layer':\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "        self.activation = nn.GELU() if act else nn.Identity()\n",
    "\n",
    "    def forward(self, x, size: Tuple[int, int]):\n",
    "        B, N, C = x.shape\n",
    "        H, W = size\n",
    "        assert N == H * W\n",
    "\n",
    "        feat = x.transpose(1, 2).view(B, C, H, W)\n",
    "        feat = self.proj(feat)\n",
    "        if self.normtype == 'batch':\n",
    "            feat = self.norm(feat).flatten(2).transpose(1, 2)\n",
    "        elif self.normtype == 'layer':\n",
    "            feat = self.norm(feat.flatten(2).transpose(1, 2))\n",
    "        else:\n",
    "            feat = feat.flatten(2).transpose(1, 2)\n",
    "        x = x + self.activation(feat)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            patch_size=16,\n",
    "            in_chans=3,\n",
    "            embed_dim=96,\n",
    "            overlapped=False):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        if patch_size[0] == 4:\n",
    "            self.proj = nn.Conv2d(\n",
    "                in_chans,\n",
    "                embed_dim,\n",
    "                kernel_size=(7, 7),\n",
    "                stride=patch_size,\n",
    "                padding=(3, 3))\n",
    "            self.norm = nn.LayerNorm(embed_dim)\n",
    "        if patch_size[0] == 2:\n",
    "            kernel = 3 if overlapped else 2\n",
    "            pad = 1 if overlapped else 0\n",
    "            self.proj = nn.Conv2d(\n",
    "                in_chans,\n",
    "                embed_dim,\n",
    "                kernel_size=to_2tuple(kernel),\n",
    "                stride=patch_size,\n",
    "                padding=to_2tuple(pad))\n",
    "            self.norm = nn.LayerNorm(in_chans)\n",
    "\n",
    "    def forward(self, x, size):\n",
    "        H, W = size\n",
    "        dim = len(x.shape)\n",
    "        if dim == 3:\n",
    "            B, HW, C = x.shape\n",
    "            x = self.norm(x)\n",
    "            x = x.reshape(B,\n",
    "                          H,\n",
    "                          W,\n",
    "                          C).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)\n",
    "        newsize = (x.size(2), x.size(3))\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        if dim == 4:\n",
    "            x = self.norm(x)\n",
    "        return x, newsize\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        k = k * self.scale\n",
    "        attention = k.transpose(-1, -2) @ v\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        x = (attention @ q.transpose(-1, -2)).transpose(-1, -2)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 ffn=True, cpe_act=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cpe = nn.ModuleList([ConvPosEnc(dim=dim, k=3, act=cpe_act),\n",
    "                                  ConvPosEnc(dim=dim, k=3, act=cpe_act)])\n",
    "        self.ffn = ffn\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = ChannelAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        if self.ffn:\n",
    "            self.norm2 = norm_layer(dim)\n",
    "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "            self.mlp = Mlp(\n",
    "                in_features=dim,\n",
    "                hidden_features=mlp_hidden_dim,\n",
    "                act_layer=act_layer)\n",
    "\n",
    "    def forward(self, x, size):\n",
    "        x = self.cpe[0](x, size)\n",
    "        cur = self.norm1(x)\n",
    "        cur = self.attn(cur)\n",
    "        x = x + self.drop_path(cur)\n",
    "\n",
    "        x = self.cpe[1](x, size)\n",
    "        if self.ffn:\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x, size\n",
    "\n",
    "\n",
    "def window_partition(x, window_size: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size: int, H: int, W: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        attn = self.softmax(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialBlock(nn.Module):\n",
    "    r\"\"\" Windows Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, window_size=7,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 ffn=True, cpe_act=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ffn = ffn\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.cpe = nn.ModuleList([ConvPosEnc(dim=dim, k=3, act=cpe_act),\n",
    "                                  ConvPosEnc(dim=dim, k=3, act=cpe_act)])\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=to_2tuple(self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        if self.ffn:\n",
    "            self.norm2 = norm_layer(dim)\n",
    "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "            self.mlp = Mlp(\n",
    "                in_features=dim,\n",
    "                hidden_features=mlp_hidden_dim,\n",
    "                act_layer=act_layer)\n",
    "\n",
    "    def forward(self, x, size):\n",
    "        H, W = size\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = self.cpe[0](x, size)\n",
    "        x = self.norm1(shortcut)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows)\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1,\n",
    "                                         self.window_size,\n",
    "                                         self.window_size,\n",
    "                                         C)\n",
    "        x = window_reverse(attn_windows, self.window_size, Hp, Wp)\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "\n",
    "        x = self.cpe[1](x, size)\n",
    "        if self.ffn:\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x, size\n",
    "\n",
    "\n",
    "\n",
    "class DaViT(nn.Module):\n",
    "    r\"\"\" Dual Attention Transformer\n",
    "    Args:\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        embed_dims (tuple(int)): Patch embedding dimension. Default: (64, 128, 192, 256)\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers. Default: (4, 8, 12, 16)\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans=3, depths=(1, 1, 3, 1), patch_size=4,\n",
    "                 embed_dims=(96, 192, 384, 768), num_heads=(3, 6, 12, 24), window_size=7, mlp_ratio=4.,\n",
    "                 qkv_bias=True, drop_path_rate=0.1, norm_layer=nn.LayerNorm, attention_types=('spatial', 'channel'),\n",
    "                 ffn=True, overlapped_patch=False, cpe_act=False, weight_init='',\n",
    "                 drop_rate=0., attn_drop_rate=0., img_size=224\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        architecture = [[index] * item for index, item in enumerate(depths)]\n",
    "        self.architecture = architecture\n",
    "        self.embed_dims = embed_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.num_stages = len(self.embed_dims)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, 2 * len(list(itertools.chain(*self.architecture))))]\n",
    "        assert self.num_stages == len(self.num_heads) == (sorted(list(itertools.chain(*self.architecture)))[-1] + 1)\n",
    "\n",
    "        self.patch_embeds = nn.ModuleList([\n",
    "            PatchEmbed(patch_size=patch_size if i == 0 else 2,\n",
    "                       in_chans=in_chans if i == 0 else self.embed_dims[i - 1],\n",
    "                       embed_dim=self.embed_dims[i],\n",
    "                       overlapped=overlapped_patch)\n",
    "            for i in range(self.num_stages)])\n",
    "\n",
    "        main_blocks = []\n",
    "        for block_id, block_param in enumerate(self.architecture):\n",
    "            layer_offset_id = len(list(itertools.chain(*self.architecture[:block_id])))\n",
    "\n",
    "            block = nn.ModuleList([\n",
    "                MySequential(*[\n",
    "                    ChannelBlock(\n",
    "                        dim=self.embed_dims[item],\n",
    "                        num_heads=self.num_heads[item],\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        drop_path=dpr[2 * (layer_id + layer_offset_id) + attention_id],\n",
    "                        norm_layer=nn.LayerNorm,\n",
    "                        ffn=ffn,\n",
    "                        cpe_act=cpe_act\n",
    "                    ) if attention_type == 'channel' else\n",
    "                    SpatialBlock(\n",
    "                        dim=self.embed_dims[item],\n",
    "                        num_heads=self.num_heads[item],\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        drop_path=dpr[2 * (layer_id + layer_offset_id) + attention_id],\n",
    "                        norm_layer=nn.LayerNorm,\n",
    "                        ffn=ffn,\n",
    "                        cpe_act=cpe_act,\n",
    "                        window_size=window_size,\n",
    "                    ) if attention_type == 'spatial' else None\n",
    "                    for attention_id, attention_type in enumerate(attention_types)]\n",
    "                ) for layer_id, item in enumerate(block_param)\n",
    "            ])\n",
    "            main_blocks.append(block)\n",
    "        self.main_blocks = nn.ModuleList(main_blocks)\n",
    "\n",
    "        # add a norm layer for each output\n",
    "        for i_layer in range(self.num_stages):\n",
    "            layer = norm_layer(self.embed_dims[i_layer])  # if i_layer != 0 else nn.Identity()\n",
    "            layer_name = f'norm{i_layer}'\n",
    "            self.add_module(layer_name, layer)\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "        if isinstance(pretrained, str):\n",
    "            self.apply(_init_weights)\n",
    "            logger = get_root_logger()\n",
    "            load_checkpoint(self, pretrained, strict=False, logger=logger)\n",
    "        elif pretrained is None:\n",
    "            self.apply(_init_weights)\n",
    "        else:\n",
    "            raise TypeError('pretrained must be a str or None')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, size = self.patch_embeds[0](x, (x.size(2), x.size(3)))\n",
    "        features = [x]\n",
    "        sizes = [size]\n",
    "        branches = [0]\n",
    "\n",
    "        for block_index, block_param in enumerate(self.architecture):\n",
    "            branch_ids = sorted(set(block_param))\n",
    "            for branch_id in branch_ids:\n",
    "                if branch_id not in branches:\n",
    "                    x, size = self.patch_embeds[branch_id](features[-1], sizes[-1])\n",
    "                    features.append(x)\n",
    "                    sizes.append(size)\n",
    "                    branches.append(branch_id)\n",
    "            for layer_index, branch_id in enumerate(block_param):\n",
    "                features[branch_id], _ = self.main_blocks[block_index][layer_index](features[branch_id], sizes[branch_id])\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_stages):\n",
    "            norm_layer = getattr(self, f'norm{i}')\n",
    "            x_out = norm_layer(features[i])\n",
    "            H, W = sizes[i]\n",
    "            out = x_out.view(-1, H, W, self.embed_dims[i]).permute(0, 3, 1, 2).contiguous()\n",
    "            outs.append(out)\n",
    "\n",
    "        return tuple(outs)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n",
    "        super(DaViT, self).train(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7c6c8166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 224, 224])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = DaViT()\n",
    "sample = torch.randn((5,3,224,224))\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4efed4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768, 7, 7])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod(sample)[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edba39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b20d71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "father = list(mod.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "af987a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (1): PatchEmbed(\n",
       "    (proj): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (2): PatchEmbed(\n",
       "    (proj): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (3): PatchEmbed(\n",
       "    (proj): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "father[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "caf53dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2, 2, 2], [3]]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e5ab8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0839f674c90357cf57b84737cbb78f6691416cdd39bc1077f945d4066e99dfa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
