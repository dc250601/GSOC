{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9c76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "#Similar to what they did in the CoAt net paper we will also make "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833a29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_embed(nn.Module):\n",
    "    def __init__(self, feature_model, img_size, channels, efn_blocks, dims):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.feature_extractor = timm.create_model(feature_model,\n",
    "                                                   features_only=True,\n",
    "                                                   out_indices=[efn_blocks])\n",
    "        \n",
    "        \n",
    "        self.feature_extractor.conv_stem = nn.Conv2d(3,   \n",
    "                                       40,\n",
    "                                       kernel_size=(3, 3),\n",
    "                                       stride=(4, 4),\n",
    "                                       padding=(1, 1),\n",
    "                                       bias=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "                # NOTE Most reliable way of determining output dims is to run forward pass\n",
    "                training = self.feature_extractor.training\n",
    "                if training:\n",
    "                    self.feature_extractor.eval()\n",
    "                o = self.feature_extractor(torch.zeros(1, channels, img_size[0], img_size[1]))\n",
    "                self.channel_output = o[0].shape[1]\n",
    "                self.feature_extractor.train(training)\n",
    "        \n",
    "        self.embed_matcher = nn.Sequential(\n",
    "            nn.Conv2d(self.channel_output, dims, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(dims, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            x = x[-1]  # last feature if backbone outputs list/tuple of features\n",
    "        x = self.embed_matcher(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fa4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_swin_effnet(nn.Module):\n",
    "    def __init__(self, feature_model = \"efficientnet_b3\",img_size = (224,224), channels = 3, efn_blocks = 2, swin_blocks = 2, no_classes = 1):\n",
    "        super().__init__()\n",
    "        assert efn_blocks + swin_blocks == 4,f\"The total no of blocks must be 4, instead {efn_blocks+swin_blocks} blocks provided \"\n",
    "        \n",
    "        self.swin_backbone = timm.create_model(\"swin_tiny_patch4_window7_224\")\n",
    "        \n",
    "        self.embeded_dim = self.swin_backbone.embed_dim * (2**(4 - swin_blocks))\n",
    "\n",
    "        self.swin_backbone.patch_embed = Hybrid_embed(feature_model = \"efficientnet_b3\",\n",
    "                                                      img_size = (224,224),\n",
    "                                                      channels = 3,\n",
    "                                                      efn_blocks = 2, \n",
    "                                                      dims = self.embeded_dim)\n",
    "        \n",
    "        #setting the first few blocks of swin to Indentity to match size\n",
    "        for i in range((4- swin_blocks)):\n",
    "            self.swin_backbone.layers[i] = nn.Identity()\n",
    "        \n",
    "        #Setting the head as per our need\n",
    "        self.swin_backbone.head = nn.Linear(self.swin_backbone.num_features, no_classes)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        return self.swin_backbone(image).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337fda6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cdipt\\anaconda3\\lib\\site-packages\\torch\\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "hybrid = Hybrid_swin_effnet()\n",
    "sample = torch.randn(5, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b969153c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid(sample).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
